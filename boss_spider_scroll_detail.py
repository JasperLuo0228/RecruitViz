import asyncio
import csv
import json
import random
import os
from playwright.async_api import async_playwright

FIELDS = ["å…³é”®è¯", "èŒä½", "èŒä½æè¿°", "å…¬å¸ä¿¡æ¯"]

class BossSpiderScroll:
    def __init__(self):
        self.cookie_file = "boss_cookies.json"
        self.output_file = "BOSSç›´è˜_ååŠæ•°æ®.csv"
        self.max_retries = 3
        self.keyword_delay = (6, 12)
        self.batch_pause = 80
        self.max_scroll = 10  # æœ€å¤šæ»‘åŠ¨å‡ æ¬¡ï¼ˆæ¯æ¬¡å¯åŠ è½½ä¸€å±ï¼‰

    async def save_cookies(self, context):
        cookies = await context.cookies()
        with open(self.cookie_file, "w") as f:
            json.dump(cookies, f)

    async def load_cookies(self, context):
        try:
            with open(self.cookie_file, "r") as f:
                await context.add_cookies(json.load(f))
            return True
        except:
            return False

    async def ensure_login(self, context):
        for attempt in range(self.max_retries):
            test_url = "https://www.zhipin.com/wapi/zpgeek/search/joblist.json?query=æµ‹è¯•&page=1"
            response = await context.request.get(test_url)
            if response.status == 200:
                try:
                    data = await response.json()
                    if data.get('zpData', {}).get('jobList') is not None:
                        return True
                except:
                    pass
            print(f"ğŸ”„ ä¼šè¯å¤±æ•ˆï¼Œæ‰«ç ç™»å½•BOSSç›´è˜ï¼ˆç¬¬{attempt+1}æ¬¡ï¼‰")
            page = await context.new_page()
            await page.goto("https://www.zhipin.com/web/geek/job?query=æµ‹è¯•")
            await page.wait_for_timeout(60000)
            await self.save_cookies(context)
            await page.close()
        return False

    async def run(self):
        async with async_playwright() as p:
            browser = await p.chromium.launch(
                headless=False,
                channel="chrome",
                args=[
                    "--disable-blink-features=AutomationControlled",
                    f"--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/{random.randint(100,113)}.0.0.0 Safari/537.36"
                ]
            )
            context = await browser.new_context(
                viewport={"width": 1366, "height": 768},
                locale="zh-CN"
            )

            if not await self.load_cookies(context) or not await self.ensure_login(context):
                print("âŒ ç™»å½•å¤±è´¥ï¼Œè¯·æ‰‹åŠ¨æ‰«ç ")
                await browser.close()
                return

            keywords = ["æ‰˜è‚²", "å®¶æ”¿", "å…»è€"]  # ç¤ºä¾‹

            file_exists = os.path.exists(self.output_file)
            f = open(self.output_file, "a", newline="", encoding="utf-8-sig")
            writer = csv.writer(f)
            if not file_exists:
                writer.writerow(FIELDS)

            all_count = 0
            for keyword in keywords:
                print(f"\nğŸ” é‡‡é›†å…³é”®è¯: {keyword}")
                page = await context.new_page()
                await page.goto(f"https://www.zhipin.com/web/geek/jobs?query={keyword}")
                await page.wait_for_timeout(2500)

                last_count = 0
                for i in range(self.max_scroll):
                    await page.mouse.wheel(0, 12000)  # å‘ä¸‹æ»šåŠ¨åˆ°åº•
                    await page.wait_for_timeout(1600)
                    cards = await page.query_selector_all("li.job-card-box, li.job-card-wrapper")
                    print(f"æ»‘åŠ¨{i+1}æ¬¡åï¼Œå…±{len(cards)}ä¸ªå²—ä½")
                    if len(cards) == last_count:
                        print(f"ğŸŸ¢ å·²ç»æ»‘åˆ°åº•ï¼Œå…±{len(cards)}ä¸ªå²—ä½ï¼Œæå‰ç»“æŸä¸‹æ‹‰")
                        break
                    last_count = len(cards)

                # ç»Ÿä¸€é‡‡é›†æ‰€æœ‰å¡ç‰‡
                cards = await page.query_selector_all("li.job-card-box, li.job-card-wrapper")
                print(f"å…±éœ€é‡‡é›†{len(cards)}ä¸ªå²—ä½")

                for idx, card in enumerate(cards):
                    try:
                        await card.click()
                        await page.wait_for_selector("div.job-detail-box, div.job-detail-body", timeout=6000)
                        await page.wait_for_timeout(500)
                    except Exception as e:
                        print(f"âš ï¸ ç¬¬{idx+1}ä¸ªå²—ä½ç‚¹å‡»å¤±è´¥ï¼š{e}")
                        continue

                    # èŒä½å
                    try:
                        job_name = await page.locator("span.job-name").first.inner_text()
                    except:
                        job_name = ""
                    # èŒä½æè¿°
                    try:
                        job_desc = await page.locator("div.job-sec-text, div.desc, p.desc").first.inner_text()
                    except:
                        job_desc = ""
                    # å…¬å¸ä¿¡æ¯
                    try:
                        boss_info = await page.locator("div.boss-info-attr").first.inner_text()
                    except:
                        boss_info = ""

                    writer.writerow([
                        keyword, job_name, job_desc, boss_info
                    ])
                    f.flush()
                    all_count += 1
                    print(f"âœ… {keyword} | å¡ç‰‡{idx+1}/{len(cards)} | æ€»{all_count}")

                    if all_count % self.batch_pause == 0:
                        print(f"â³ è¾¾åˆ°{self.batch_pause}æ¡ï¼Œè‡ªåŠ¨ä¼‘æ¯8ç§’")
                        await asyncio.sleep(8)
                await page.close()
                await asyncio.sleep(random.uniform(*self.keyword_delay))
            f.close()
            await browser.close()
            print("\nğŸ‰ é‡‡é›†å®Œæˆï¼æ•°æ®å·²å®æ—¶ä¿å­˜")

if __name__ == "__main__":
    spider = BossSpiderScroll()
    asyncio.run(spider.run())
